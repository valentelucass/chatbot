## Deploy na Vercel

Este projeto estÃ¡ preparado para deploy na Vercel com backend em FastAPI e frontend estÃ¡tico.

### Requisitos
- Python (gerenciado pela Vercel via `@vercel/python`)
- `requirements.txt` no raiz do projeto
- `vercel.json` jÃ¡ configurado
- VariÃ¡vel de ambiente `OPENAI_API_KEY`

### Passos
1. Crie um projeto na Vercel e importe este repositÃ³rio.
2. Em Settings â†’ Environment Variables, adicione:
   - `OPENAI_API_KEY` (Production/Preview/Development conforme desejar)
3. Deploy.

### Roteamento
- API: `api/main.py` servida por `@vercel/python`.
- Frontend: tudo em `frontend/` servindo assets estÃ¡ticos.
- Rotas definidas em `vercel.json`:
  - `/api/(.*)` â†’ `api/main.py`
  - `/*.(css|js|png|jpg|jpeg|svg|ico|webp|json)` â†’ `frontend/`
  - fallback `/(.*)` â†’ `frontend/index.html`

### Endpoints Ãºteis
- `POST /api/chat` â†’ endpoint principal do chatbot
- `GET /api/kb_status` â†’ diagnÃ³stico da base local (quantidade, mtime, caminho)

## Desenvolvimento local

### Backend
```bash
python -m api.main
# ServirÃ¡ em http://127.0.0.1:8000
```

### Frontend
Abra `frontend/index.html` com Live Server (http://127.0.0.1:5500). O frontend detecta ambiente local e chama `http://127.0.0.1:8000/api/chat` automaticamente.

## VariÃ¡veis de ambiente
- `OPENAI_API_KEY` â†’ chave da OpenAI usada no fallback do modelo.

## Prioridade da Base Local
A lÃ³gica tenta responder primeiro usando `api/chatbot.json` (match exato e por substring). SÃ³ chama a OpenAI se nÃ£o houver resposta local.

## Recursos Implementados
- Auto-reload do `chatbot.json` por mtime.
- Toggle de resposta curta/longa no frontend (enviado como `mode`).
- "Mostrar mais/menos" para respostas longas.
- Endpoint de diagnÃ³stico `GET /api/kb_status`.

### Novidades
- Streaming de respostas: `POST /api/chat_stream` (melhor UX em respostas longas).
- Realce de cÃ³digo com Prism.js no frontend (`frontend/index.html`).
- CorrespondÃªncia local aprimorada:
  - Exato â†’ Substring â†’ Fuzzy (SequenceMatcher + Jaccard) â†’ Embeddings (OpenAI) opcional.

## Como funciona o matching local
Ordem de tentativa no `api/chatbot.py` â†’ `get_local_response()`:
1. Exato: `normalize_text(input) == normalize_text(keyword)`.
2. Substring: `keyword âˆˆ input normalizado`.
3. Fuzzy: combinaÃ§Ã£o de similaridade de sequÃªncia e sobreposiÃ§Ã£o de tokens (limiar ~0.78).
4. Embeddings (opcional): usa `text-embedding-3-small` para comparar consulta com vetores da KB (limiar cos ~0.82). Cache de embeddings Ã© invalidado ao recarregar a base.

ObservaÃ§Ã£o: Embeddings sÃ³ ativam se `OPENAI_API_KEY` estiver definido.

## Streaming no Frontend
O frontend agora consome `/api/chat_stream` e renderiza o texto incrementalmente; ao final aplica Markdown + Prism.js e o colapso "Mostrar mais/menos".

## Realce de CÃ³digo (Prism.js)
IncluÃ­mos Prism theme Tomorrow e linguagens comuns (JS, TS, Python, Java). Para detectar linguagem, use blocos:

```markdown
```python
print("hello")
```
```

O renderer adiciona `class="language-<lang>"` ao `<code>`.
# Chatbot para Estudantes

Este projeto Ã© um chatbot simples e escalÃ¡vel, projetado para ajudar estudantes com dÃºvidas sobre programaÃ§Ã£o e outros tÃ³picos de estudo. A arquitetura foi pensada para ser de fÃ¡cil manutenÃ§Ã£o, baixo custo e com deploy simplificado na Vercel.

## âœ¨ VisÃ£o Geral da Arquitetura

O projeto Ã© dividido em duas camadas principais:

-   **Frontend**: Uma interface de chat estÃ¡tica construÃ­da com HTML, CSS e JavaScript puros. Ela Ã© responsÃ¡vel por exibir a conversa e se comunicar com o backend.
-   **Backend**: Uma API em Python construÃ­da com o framework FastAPI. Ela contÃ©m a lÃ³gica do chatbot, processando as mensagens dos usuÃ¡rios.

### Fluxo de Funcionamento
1.  O usuÃ¡rio envia uma mensagem atravÃ©s do frontend.
2.  O frontend faz uma requisiÃ§Ã£o HTTP POST para o endpoint `/api/chat`.
3.  O backend FastAPI recebe a mensagem.
4.  A lÃ³gica do chatbot primeiro busca por uma resposta em uma base de conhecimento local (`chatbot.json`).
5.  Se nenhuma resposta correspondente for encontrada, ele consulta a API da OpenAI (ChatGPT) como fallback.
6.  A resposta Ã© retornada em formato JSON para o frontend, que a exibe na tela.

---

## ğŸš€ Estrutura de DiretÃ³rios

```
chatbot/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py             # Entrada da API (FastAPI)
â”‚   â”œâ”€â”€ chatbot.py          # MÃ³dulo de lÃ³gica do chatbot
â”‚   â”œâ”€â”€ chatbot.json        # Base local de conhecimento
â”‚   â”œâ”€â”€ config.py           # ConfiguraÃ§Ãµes e variÃ¡veis de ambiente
â”‚   â”œâ”€â”€ utils.py            # FunÃ§Ãµes auxiliares
â”‚   â””â”€â”€ __init__.py         # Marca 'api' como pacote Python
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html          # Estrutura da pÃ¡gina de chat
â”‚   â”œâ”€â”€ style.css           # Estilos visuais
â”‚   â””â”€â”€ script.js           # LÃ³gica do cliente (chamadas de API)
â”‚
â”œâ”€â”€ requirements.txt        # DependÃªncias do backend Python
â”œâ”€â”€ vercel.json             # ConfiguraÃ§Ã£o de deploy para a Vercel
â”œâ”€â”€ .env                    # VariÃ¡veis de ambiente (nÃ£o versionar)
â”œâ”€â”€ .gitignore              # Arquivos/pastas ignorados pelo Git
â””â”€â”€ README.md               # Este arquivo
```

---

## ğŸ› ï¸ Como Executar Localmente

Siga os passos abaixo para testar o projeto na sua mÃ¡quina.

### PrÃ©-requisitos
-   [Python 3.9+](https://www.python.org/downloads/)
-   Um editor de cÃ³digo (ex: VS Code)
-   Conta na [OpenAI](https://platform.openai.com/) e uma API Key (opcional, apenas para o fallback com ChatGPT).

### Passo a Passo no Terminal

1.  **Clone o repositÃ³rio (ou crie a pasta e os arquivos):**
    ```bash
    # Se estiver usando git
    git clone <url-do-seu-repositorio>
    cd chatbot
    ```

2.  **Crie e ative um ambiente virtual:**
    Isso isola as dependÃªncias do seu projeto.
    ```bash
    # Windows
    python -m venv .venv
    .\.venv\Scripts\activate

    # macOS / Linux
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Instale as dependÃªncias Python:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **(Opcional) Configure as variÃ¡veis de ambiente para o fallback (OpenAI):**
    Crie um arquivo chamado `.env` na raiz (`chatbot/`) e adicione sua chave da OpenAI.
    ```
    # .env
    OPENAI_API_KEY="sk-...sua_chave_aqui"
    ```

5.  **Inicie o servidor da API:**
    ```bash
    uvicorn api.main:app --reload
    ```
    -   O servidor estarÃ¡ rodando em `http://127.0.0.1:8000`.
    -   A flag `--reload` reinicia o servidor automaticamente quando vocÃª altera o cÃ³digo.

6.  **Abra o frontend:**
    -   Navegue atÃ© a pasta `frontend/`.
    -   Abra o arquivo `index.html` diretamente no seu navegador.
    -   **Importante:** Para o `script.js` conseguir chamar a API localmente, vocÃª pode precisar de um servidor de arquivos estÃ¡ticos simples para evitar problemas de CORS. A forma mais fÃ¡cil Ã© usar a extensÃ£o "Live Server" no VS Code.

---

## ğŸŒ Deploy na Vercel

O deploy deste projeto na Vercel Ã© extremamente simples:

1.  Envie seu projeto para um repositÃ³rio no GitHub, GitLab ou Bitbucket.
2.  Acesse sua conta na [Vercel](https://vercel.com).
3.  Clique em "Add New..." -> "Project".
4.  Importe o repositÃ³rio do seu projeto.
5.  A Vercel deve detectar automaticamente que Ã© um projeto Python com FastAPI e usar as configuraÃ§Ãµes do `vercel.json`. Nenhum ajuste Ã© necessÃ¡rio.
6.  **(Opcional) Adicione suas Environment Variables** na aba de configuraÃ§Ãµes do projeto na Vercel (ex: `OPENAI_API_KEY`).
7.  Clique em "Deploy". Pronto!

---

## ğŸ“ ObservaÃ§Ãµes importantes

-   **CORS/localhost**: o backend permite origens `http://127.0.0.1:5500` e `http://localhost:5500`. Use um servidor estÃ¡tico (ex.: extensÃ£o Live Server no VS Code) para abrir `frontend/index.html` durante o desenvolvimento.
-   **Endpoint dinÃ¢mico no frontend**: `frontend/script.js` seleciona automaticamente o endpoint da API. Em localhost usa `http://127.0.0.1:8000/api/chat`; em produÃ§Ã£o usa o caminho relativo `/api/chat`.
-   **.gitignore**: jÃ¡ incluso para evitar versionar `.venv/`, `.env`, caches, logs e artefatos de build.